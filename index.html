<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Information Geometry Course</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>

<header>
  <h1>Information Geometry</h1>
  <p>Taught by Herlock Rahimi | Yale University</p>
</header>

<section id="about">
  <h2>About the Course</h2>
  <p>
    This graduate mini-course explores the differential geometric structure of probability distributions. Key topics include information geometry, exponential families, optimal transport, statistical estimation, and their applications to learning, optimization, and reinforcement learning.
  </p>
</section>

<section id="sessions">
  <h2>Course Sessions</h2>

  <div class="session">
    <h3>Session 1: Information, Estimation, and Geometry</h3>
    <p>Introduction to entropy, KL divergence, and estimation techniques (MLE, KL, Wasserstein). Covers differential geometry basics like tangent vectors, Riemannian metrics, and optimization on curved spaces.</p>
    <iframe 
  width="560" 
  height="315"
  src="https://www.youtube.com/embed/BxCrIVmzJmc?list=PLSPy7-TwtzXV7Mc2Q2L5q-T1R1jEG8uzr" 
  title="Information Geometry Lecture 1" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe><a href="assets/slides/session1.pdf" download>Download Slides (PDF)</a>
  </div>

  <div class="session">
    <h3>Session 2: Gaussian Mixture Models and Geometric Tools</h3>
    <p>Two-Gaussian EM and non-EM estimation, smooth manifolds, connections, Bregman divergences, and dual coordinate systems including their application to mirror descent and RL.</p>
    <iframe src="https://www.youtube.com/watch?v=2iCidP74ga8&list=PLSPy7-TwtzXV7Mc2Q2L5q-T1R1jEG8uzr&index=2" allowfullscreen></iframe>
    <a href="assets/slides/session2.pdf" download>Download Slides (PDF)</a>
  </div>

  <div class="session">
    <h3>Session 3: Dual Connections, Divergences, and Dually Flat Spaces</h3>
    <p>Explores α-connections, divergence-induced metrics, Bregman divergence, and Legendre duality. Canonical divergences and the Pythagorean theorem in dually flat spaces are introduced.</p>
    <iframe src="https://www.youtube.com/watch?v=THWMtEWe85E&list=PLSPy7-TwtzXV7Mc2Q2L5q-T1R1jEG8uzr&index=3" allowfullscreen></iframe>
    <a href="assets/slides/session3.pdf" download>Download Slides (PDF)</a>
  </div>

  <div class="session">
    <h3>Session 4: EM Algorithm, Natural Gradient, and Statistical Geometry</h3>
    <p>Describes Fisher metric, dual connections, KL divergence geometry, EM algorithm as alternating projections, and natural gradient methods for optimization and learning.</p>
    <iframe src="https://www.youtube.com/watch?v=BxCrIVmzJmc&list=PLSPy7-TwtzXV7Mc2Q2L5q-T1R1jEG8uzr&index=4" allowfullscreen></iframe>
    <a href="assets/slides/session4.pdf" download>Download Slides (PDF)</a>
  </div>

  <div class="session">
    <h3>Session 5: Optimal Transport Meets Information Geometry</h3>
    <p>Introduces Monge and Kantorovich formulations, Wasserstein distances, Otto calculus, displacement geodesics, and contrasts these with Fisher geometry to develop a unified view.</p>
    <iframe src="https://www.youtube.com/embed/VIDEO_ID_5" allowfullscreen></iframe>
    <a href="assets/slides/session5.pdf" download>Download Slides (PDF)</a>
  </div>
</section>

<footer>
  <p>© 2025 Herlock Rahimi | <a href="https://github.com/your-username/information-geometry-course">GitHub Repository</a></p>
</footer>

</body>
</html>

